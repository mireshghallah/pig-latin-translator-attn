{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Local imports\n",
    "import utils\n",
    "import data_handler\n",
    "from data_handler import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options/ Hyperparameters required to train and test the model\n",
    "opts = EasyDict()\n",
    "\n",
    "opts.n_epochs = 1000\n",
    "opts.batch_size = 64\n",
    "opts.learning_rate = 0.0001\n",
    "opts.lr_decay = 0.99\n",
    "opts.hidden_layer_size = 100\n",
    "opts.model_name = \"simple_rnn\"\n",
    "opts.checkpoints_dir = \"./checkpoints/\"+opts.model_name \n",
    "\n",
    "TEST_SENTENCE = 'i love deep learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_dir_if_not_exists(opts.checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_pairs, vocab_size, idx_dict = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the line pairs into 8:2, train and val split\n",
    "num_lines = len(line_pairs)\n",
    "num_train = int(0.8 * num_lines)\n",
    "train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = create_dict(train_pairs)\n",
    "val_dict = create_dict(val_pairs)\n",
    "\n",
    "# Study the structure of the created train_dict and val_dict variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyGRUCell, self).__init__()\n",
    "        #TODO: zero intit, weight return\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_z=nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        self.weight_r=nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        self.weight = nn.Linear(input_size + hidden_size , hidden_size)\n",
    "        #self.weight_z = torch.zeros(input_size + hidden_size , hidden_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.weight_z)\n",
    "        #self.weight_r = torch.zeros(input_size + hidden_size , hidden_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.weight_r)\n",
    "        #self.weight = torch.zeros(input_size + hidden_size , hidden_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.weight)\n",
    "        #bias\n",
    "        #self.bias_z = torch.zeros(1 , hidden_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.bias_z,)\n",
    "        #self.bias_r = torch.zeros(1 , hidden_size,requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.bias_r)\n",
    "        #self.bias = torch.zeros(1 , hidden_size,requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.bias)\n",
    "    \n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Forward pass of the GRU computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "        #print h_prev.shape , x.shape, \"thiiiiis\"\n",
    "        input_concatenated = torch.cat((h_prev , x), 1)\n",
    "        z = torch.sigmoid(self.weight_z(input_concatenated))\n",
    "        r = torch.sigmoid(self.weight_r(input_concatenated))\n",
    "        g = torch.tanh(self.weight(torch.cat(((r*h_prev),x),1)))\n",
    "        h_new = ((1-z)*h_prev)+ (z*g)\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        # z = ...\n",
    "        # r = ...\n",
    "        # g = ...\n",
    "        # h_new = ...\n",
    "        return h_new \n",
    "   # def parameters(self):\n",
    "       # return[self.weight_z,self.weight_r, self.weight]\n",
    "\n",
    "# Implement your Encoder RNN using instances of GRU Cell you just created.\n",
    "# You would need a character embedding layer for this.\n",
    "# Implement your Encoder RNN using instances of GRU Cell you just created.\n",
    "# You would need a character embedding layer for this.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = MyGRUCell(self.vocab_size, self.hidden_size)\n",
    "    \n",
    "        # embedding layer\n",
    "        # GRU cell\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        inp_onehot = torch.FloatTensor(batch_size, self.vocab_size, seq_len)\n",
    "        inp_onehot.zero_()\n",
    "        #print(inputs.size())\n",
    "        #print(inp_onehot.size())\n",
    "        #print(torch.unsqueeze(inputs,1).size())\n",
    "        embed=inp_onehot.scatter_(1, (torch.unsqueeze(inputs,1).type(torch.LongTensor)), 1)\n",
    "        embed_unstack = torch.unbind(embed,2)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        annotations = []\n",
    "        for i in range (seq_len):\n",
    "            #print i\n",
    "            #print embed_unstack[i].shape , hidden.shape\n",
    "    \n",
    "            hidden = self.cell.forward(embed_unstack[i], hidden)\n",
    "            annotations.append(hidden)\n",
    "        #print (annotations.size())\n",
    "            \n",
    "        # The encoded embeddings should be of size batch_size x seq_len x hidden_size\n",
    "        # Complete the forward pass \n",
    "        # ....\n",
    "        \n",
    "        return annotations, hidden\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
    "        of a batch of sequences.\n",
    "\n",
    "        Arguments:\n",
    "            bs: The batch size for the initial hidden state.\n",
    "\n",
    "        Returns:\n",
    "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        return torch.zeros(bs, self.hidden_size)\n",
    "   # def parameters(self):\n",
    "       # return self.cell.parameters()\n",
    "\n",
    "# Implement your Decoder RNN using instances of GRU Cell you just created.\n",
    "# You would need a character embedding layer for this. \n",
    "# In addition you would also require an activation function applied to the output of the GRU Cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #self.weight = torch.zeros(hidden_size , vocab_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.weight)\n",
    "        #self.bias = torch.zeros(1 , vocab_size, requires_grad=True)\n",
    "        #torch.nn.init.xavier_uniform_(self.bias)\n",
    "        self.weight= nn.Linear(hidden_size,vocab_size)\n",
    "        self.cell = MyGRUCell (self.vocab_size, self.hidden_size)\n",
    "        \n",
    "        # define embedding layer\n",
    "        # define GRU cell\n",
    "        # define out\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Forward pass of the decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            x: Input token indexes across a batch for a single time step. (batch_size x 1)\n",
    "            h_prev: The hidden states from the previous step, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch. (batch_size x vocab_size)\n",
    "            h_new: The new hidden states, across a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        inp_onehot = torch.FloatTensor(batch_size, self.vocab_size)\n",
    "        inp_onehot.zero_()\n",
    "        #print(inp_onehot.size())\n",
    "        #print(\"x size\", x.size())\n",
    "        embed=inp_onehot.scatter_(1, x.type(torch.LongTensor), 1)\n",
    "        #print h_prev.size()\n",
    "        #print(\"embed size\" ,embed.size())\n",
    "        h_new = self.cell.forward(embed,h_prev)\n",
    "        output = self.weight(h_new) ##check this!\n",
    "        # Implement the forward pass of this network\n",
    "        # ....\n",
    "        return output, h_new\n",
    "   # def parameters(self):\n",
    "    #    return self.cell.parameters(), [self.weight, self.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "### Setup: Create Encoder, Decoder Objects ###\n",
    "##########################################################################\n",
    "encoder = Encoder(vocab_size=vocab_size, hidden_size=opts.hidden_layer_size)\n",
    "decoder = Decoder(vocab_size=vocab_size, hidden_size=opts.hidden_layer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dict, val_dict, idx_dict, encoder, decoder, opts):\n",
    "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
    "        * Prints training and val loss each epoch.\n",
    "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
    "\n",
    "    Arguments:\n",
    "        train_dict: The training word pairs, organized by source and target lengths.\n",
    "        val_dict: The validation word pairs, organized by source and target lengths.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model to generate output tokens.\n",
    "        opts: The input arguments for hyper-parameters and others.\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params_enc=encoder.parameters()\n",
    "    params_dec=decoder.parameters()\n",
    "    #print(params.size())\n",
    "    #decoder_param1= decoder.parameters()\n",
    "    #print(encoder1.parameters().size())\n",
    "    optimizer = optim.Adam(list(params_enc)+list(params_dec), lr=opts.learning_rate) \n",
    "\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    loss_log = open(os.path.join(opts.checkpoints_dir, 'loss_log.txt'), 'w')\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    key=(11, 16)\n",
    "    #print(train_dict[key])\n",
    "    train_dict2 = {key: train_dict[key]}\n",
    "    for epoch in range(opts.n_epochs ):\n",
    "\n",
    "            # decay the learning rate of the optimizer\n",
    "            # ....\n",
    "    #         optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
    "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
    "        epoch_losses = []\n",
    "        for key in train_dict:\n",
    "            #print (train_dict[key])\n",
    "            #print(key)\n",
    "            #print(train_dict2[key])\n",
    "\n",
    "            input_strings, target_strings = zip(*train_dict[key])\n",
    "\n",
    "            # Make your input tensor and the target tensors\n",
    "            # HINT : use the function string_to_index_list given in data_handler.py\n",
    "            # input_tensors = ....\n",
    "            # output_tensors = ....\n",
    "            input_tensors = [data_handler.string_to_index_list(string, char_to_index, end_token) for string in input_strings]\n",
    "            output_tensors = [data_handler.string_to_index_list(string, char_to_index, end_token) for string in target_strings]\n",
    "            input_tensors = torch.Tensor(input_tensors)\n",
    "            output_tensors = torch.Tensor(output_tensors)\n",
    "            num_tensors = len(input_tensors)\n",
    "            num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                #print (i)\n",
    "                start = i * opts.batch_size\n",
    "                end = start + opts.batch_size\n",
    "\n",
    "                # Define inputs and targets for THIS batch, beginning at index 'start' to 'end'\n",
    "                # inputs = ....\n",
    "                # outputs = ....\n",
    "                inputs = torch.Tensor(input_tensors[start:end])\n",
    "                outputs = torch.Tensor(output_tensors[start:end])\n",
    "                # The batch size may be different in each epoch\n",
    "                BS = inputs.size(0)\n",
    "\n",
    "                encoder_annotations, encoder_hidden = encoder.forward(inputs)\n",
    "\n",
    "                # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "                # decoder_hidden = ....\n",
    "                decoder_hidden = encoder_hidden\n",
    "                # Define the first decoder input. This would essentially be the start_token\n",
    "                # decoder_input = ....\n",
    "                decoder_input = torch.Tensor([start_token for i in range(BS)])\n",
    "                #print(\"decoder sized 1\",decoder_input.size())\n",
    "                decoder_input = decoder_input.unsqueeze(1)\n",
    "                #print(\"decoder sized\",decoder_input.size())\n",
    "                loss = 0.0\n",
    "\n",
    "                seq_len = outputs.size(1)  # Gets seq_len from BS x seq_len\n",
    "                for i in range(seq_len):\n",
    "                    #print (\"this\" , i)\n",
    "                    #print(decoder_hidden.size())\n",
    "                    decoder_output, decoder_hidden = decoder.forward(decoder_input, decoder_hidden)\n",
    "\n",
    "                    current_target = outputs[:,i]\n",
    "                    #print(current_target)\n",
    "                            # Calculate the cross entropy between the decoder distribution and Ground truth (current_target)\n",
    "                            # loss += ....\n",
    "                    #print(criterion(decoder_output, current_target.type(torch.LongTensor)))\n",
    "                    loss += criterion(decoder_output, current_target.type(torch.LongTensor))\n",
    "                            # Find out the most probable character (ni) from the softmax distribution produced\n",
    "                            # ni = ....\n",
    "\n",
    "                    decoder_input = outputs[:,i].unsqueeze(1)\n",
    "\n",
    "                loss /= float(seq_len)\n",
    "                #print(loss, \"loss\")\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "                        # Compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                        # Update the parameters of the encoder and decoder\n",
    "                optimizer.step()\n",
    "\n",
    "        train_loss = np.mean(epoch_losses)\n",
    "        val_loss = evaluate(val_dict, encoder, decoder, idx_dict, criterion, opts)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            utils.store_checkpoints(encoder, decoder, idx_dict, opts)\n",
    "\n",
    "        gen_string = find_pig_latin(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
    "        #gen_string=\"hello\"\n",
    "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
    "\n",
    "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
    "        loss_log.flush()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        utils.store_loss_plots(train_losses, val_losses, opts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_dict, encoder, decoder, idx_dict, criterion, opts):\n",
    "    \"\"\"Evaluates the model on a held-out validation or test set. \n",
    "    This should be pretty straight-forward if you have figured out how to do the training correctly.\n",
    "    From then, it's just copy and paste.\n",
    "\n",
    "    Arguments:\n",
    "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model to generate output tokens.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss: The average loss over all batches from data_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for key in data_dict:\n",
    "\n",
    "        input_strings, target_strings = zip(*data_dict[key])\n",
    "        # Make your input tensor and the target tensors\n",
    "        # HINT : use the function string_to_index_list given in data_handler.py\n",
    "        # input_tensors = ....\n",
    "        # output_tensors = ....\n",
    "        input_tensors = [data_handler.string_to_index_list(string, char_to_index, end_token) for string in input_strings]\n",
    "        output_tensors = [data_handler.string_to_index_list(string, char_to_index, end_token) for string in target_strings]\n",
    "        input_tensors = torch.Tensor(input_tensors)\n",
    "        output_tensors = torch.Tensor(output_tensors)\n",
    "        num_tensors = len(input_tensors)\n",
    "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start = i * opts.batch_size\n",
    "            end = start + opts.batch_size\n",
    "\n",
    "            # Define inputs and targets for THIS batch, beginning at index 'start' to 'end'\n",
    "            # inputs = ....\n",
    "            # outputs = ....\n",
    "            inputs = torch.Tensor(input_tensors[start:end])\n",
    "            outputs = torch.Tensor(output_tensors[start:end])\n",
    "            # The batch size may be different in each epoch\n",
    "            BS = inputs.size(0)\n",
    "\n",
    "            encoder_annotations, encoder_hidden = encoder.forward(inputs)\n",
    "            \n",
    "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "            # decoder_hidden = ....\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Define the first decoder input. This would essentially be the start_token\n",
    "            # decoder_input = ....\n",
    "            decoder_input = torch.Tensor([start_token for i in range(BS)])\n",
    "            #print(\"decoder sized 1\",decoder_input.size())\n",
    "            decoder_input = decoder_input.unsqueeze(1)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            seq_len = outputs.size(1)  # Gets seq_len from BS x seq_len\n",
    "\n",
    "            for i in range(seq_len):\n",
    "                decoder_output, decoder_hidden= decoder.forward(decoder_input, decoder_hidden)\n",
    "\n",
    "                current_target = outputs[:,i]\n",
    "\n",
    "                # Calculate the cross entropy between the decoder distribution and Ground truth (current_target)\n",
    "                # loss += ....\n",
    "                loss += criterion(decoder_output, current_target.type(torch.LongTensor))\n",
    "                # Find out the most probable character (ni) from the softmax distribution produced\n",
    "                # ni = ....\n",
    "                \n",
    "                # Update decoder_input at the next time step to be this time-step's target \n",
    "                # decoder_input = ....\n",
    "                decoder_output=torch.softmax(decoder_output.float(),1)\n",
    "                _, indices = torch.max(decoder_output, 1)\n",
    "                #indices=torch.unsqueeze(indices,1)\n",
    "                #inp_onehot = torch.FloatTensor(BS, vocab_size)\n",
    "                #inp_onehot.zero_()\n",
    "                #print(inp_onehot.size())\n",
    "                #print(\"x size\", x.size())\n",
    "                #decoder_input=inp_onehot.scatter_(1, indices.type(torch.LongTensor), 1)\n",
    "                decoder_input = outputs[:,i].unsqueeze(1)\n",
    "                \n",
    "            loss /= float(seq_len)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pig_latin(sentence, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
    "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
    "    word independently, and then stitching the words back together with spaces between them.\n",
    "    \"\"\"\n",
    "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
    "\n",
    "\n",
    "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a given string from English to Pig-Latin.\n",
    "    Not much to do here as well. Follows basically the same structure as that of the function evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "    index_to_char = idx_dict['index_to_char']\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = ''\n",
    "    #print input_string\n",
    "    # convert given string to an array of indexes\n",
    "    # HINT: use the function string_to_index_list provided in data_handler\n",
    "    # indexes = ....\n",
    "    indexes = data_handler.string_to_index_list(input_string, char_to_index, end_token)\n",
    "    #print(input_string)\n",
    "    indexes =(torch.Tensor(indexes))\n",
    "    indexes =indexes.unsqueeze(0)\n",
    "    #print(indexes.size(), \"index size\")\n",
    "    encoder_annotations, encoder_last_hidden = encoder.forward(torch.Tensor(indexes))\n",
    "    #print(torch.Tensor(encoder_last_hidden).size())\n",
    "    # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "    # decoder_hidden = ....\n",
    "\n",
    "    # Define the first decoder input. This would essentially be the start_token\n",
    "    # decoder_input = ....\n",
    "    decoder_hidden = encoder_last_hidden\n",
    "            # Define the first decoder input. This would essentially be the start_token\n",
    "            # decoder_input = ....\n",
    "    #print (decoder_hidden.size(), \"dec size\")\n",
    "    decoder_input = torch.Tensor([start_token for i in range(1)])\n",
    "    \n",
    "    decoder_input=decoder_input.unsqueeze(1)\n",
    "    #print(decoder_input.size(), \"size\")\n",
    "            #print(\"decoder sized 1\",decoder_input.size())\n",
    "    for i in range(max_generated_chars):\n",
    "        decoder_output, decoder_hidden= decoder.forward(decoder_input, decoder_hidden)\n",
    "    \n",
    "        # Calculate the cross entropy between the decoder distribution and Ground truth (current_target)\n",
    "        # loss += ....\n",
    "        \n",
    "        # Find out the most probable character (ni) from the softmax distribution produced\n",
    "        # ni = ....\n",
    "        _, indices = torch.max(decoder_output, 1)\n",
    "        \n",
    "        indices=indices.unsqueeze(1)\n",
    "        #print(indices.size(), \"size2\")\n",
    "        #indices=torch.unsqueeze(indices,1)\n",
    "        inp_onehot = torch.FloatTensor(1, vocab_size)\n",
    "        inp_onehot.zero_()\n",
    "                #print(inp_onehot.size())\n",
    "        #print(\"x size\", x.size())\n",
    "        decoder_input=inp_onehot.scatter_(1, indices.type(torch.LongTensor), 1)\n",
    "        #print(indices[0][0].item())\n",
    "        ni = index_to_char[indices[0][0].item()]\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string += ni\n",
    "            \n",
    "            # update decoder_input at the next time step to be ni \n",
    "            # decoder_input = ....\n",
    "\n",
    "    return gen_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 3.259 | Val loss: 3.076 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
      "Epoch:   1 | Train loss: 2.917 | Val loss: 2.845 | Gen: aayyyyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eayyyyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eayyyyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS iayyyyyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:   2 | Train loss: 2.785 | Val loss: 2.718 | Gen: aaaayyyyyyyyyyyyyyyy eaaaayyyyyyyyyyyyyyy eaaayyyyyyyyyyyyyyyy eeaaayyyyyyyyyyyyyyy\n",
      "Epoch:   3 | Train loss: 2.670 | Val loss: 2.675 | Gen: aayyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eaayyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eaayyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eeaayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:   4 | Train loss: 2.648 | Val loss: 2.527 | Gen: aayyyaaaaaaaaaaaaaaa eayyyaaaaaaaaaaaaaaa aayyyaaaaaaaaaaaaaaa eayyyaaaaaaaaaaaaaaa\n",
      "Epoch:   5 | Train loss: 2.499 | Val loss: 2.384 | Gen: aayyyyyyyyyyyyyyyyyy eayyayyyyyyyyyyyyyyy eayyayyyyyyyyyyyyyyy eayyyyyyyyyyyyyyyyyy\n",
      "Epoch:   6 | Train loss: 2.312 | Val loss: 2.205 | Gen: eyyEOSiyyyyyyyyyyyyyyy eyyiiyyyyyyyyyyyyyyy eyyiiyyyyyyyyyyyyyyy eyyiiyyyyyyyyyyyyyyy\n",
      "Epoch:   7 | Train loss: 2.187 | Val loss: 2.107 | Gen: ayEOSiiyEOSyyyyyyyyyyyyy oyEOSiyyyyyyyyyyyyyyyy oyEOSiyyyyyyyyyyyyyyyy oyEOSiyyyyyyyyyyyyyyyy\n",
      "Epoch:   8 | Train loss: 2.100 | Val loss: 2.034 | Gen: ayEOSyyEOSyyyyyyyyyyyyyy eyEOSiyyyyyyyyyyyyyyyy eyEOSiyEOSyyyyyyyyyyyyyy eeiiyyyyyyyyyyyyyyyy\n",
      "Epoch:   9 | Train loss: 2.040 | Val loss: 1.987 | Gen: ayEOSyyEOSyyEOSyEOSyyEOSyyyyyy ayEOSyyEOSyEOSEOSyEOSEOSyEOSyyyyyy ayEOSyyEOSyEOSEOSyEOSEOSyEOSyyyyyy ay-yyEOSyEOSEOSyEOSEOSyEOSyyyyyy\n",
      "Epoch:  10 | Train loss: 1.976 | Val loss: 1.938 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ay--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oi-------EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  11 | Train loss: 1.924 | Val loss: 1.932 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyy--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ayyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyy---EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  12 | Train loss: 1.893 | Val loss: 1.904 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyy--------EOSEOSEOSEOSEOSEOSEOSEOSEOS eyy----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eey--------------EOSEOSEOS\n",
      "Epoch:  13 | Train loss: 1.851 | Val loss: 1.885 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy------EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ayy-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyy---------EOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  14 | Train loss: 1.846 | Val loss: 1.859 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy--------------EOSEOSEOSEOS ayy--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ii------------------\n",
      "Epoch:  15 | Train loss: 1.812 | Val loss: 1.833 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy------------------ ey------------------ eil-----------------\n",
      "Epoch:  16 | Train loss: 1.765 | Val loss: 1.823 | Gen: ayEOSEOSEOSEOSEOS-EOS-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy------------------ ay-y--EOS------------- ey------------------\n",
      "Epoch:  17 | Train loss: 1.746 | Val loss: 1.776 | Gen: ayEOSEOSEOS--------------- oy------------------ ey-y---------------- ey------------------\n",
      "Epoch:  18 | Train loss: 1.722 | Val loss: 1.756 | Gen: ayEOS-EOS--------------- oy------------------ ey-y---------------- ey------------------\n",
      "Epoch:  19 | Train loss: 1.662 | Val loss: 1.744 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y---------------- ey-y--EOS-----EOSEOSEOSEOSEOSEOSEOSEOS iy-y----------------\n",
      "Epoch:  20 | Train loss: 1.669 | Val loss: 1.699 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y---------EOSEOSEOSEOSEOSEOSEOS ey-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y-------EOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  21 | Train loss: 1.607 | Val loss: 1.668 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y---------------EOS ey-y--------EOSEOSEOSEOSEOSEOSEOSEOS ey-y-----------EOSEOSEOSEOSEOS\n",
      "Epoch:  22 | Train loss: 1.578 | Val loss: 1.650 | Gen: ayEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y---EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  23 | Train loss: 1.554 | Val loss: 1.617 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-yEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  24 | Train loss: 1.521 | Val loss: 1.599 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-------EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-yEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  25 | Train loss: 1.480 | Val loss: 1.563 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  26 | Train loss: 1.452 | Val loss: 1.542 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-y---EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  27 | Train loss: 1.416 | Val loss: 1.528 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y-----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-yEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny------EOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  28 | Train loss: 1.394 | Val loss: 1.488 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-yEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny-----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  29 | Train loss: 1.343 | Val loss: 1.471 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy-y--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny----EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  30 | Train loss: 1.326 | Val loss: 1.444 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy---EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny------EOSEOSEOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  31 | Train loss: 1.287 | Val loss: 1.404 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny--------EOSEOSEOSEOSEOSEOSEOSEOS\n",
      "Epoch:  32 | Train loss: 1.261 | Val loss: 1.398 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny-----------EOSEOSEOSEOSEOS\n",
      "Epoch:  33 | Train loss: 1.219 | Val loss: 1.346 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oy--EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny-y------------EOSEOS\n",
      "Epoch:  34 | Train loss: 1.192 | Val loss: 1.327 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOS-EOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny-----------EOSEOSEOSEOSEOS\n",
      "Epoch:  35 | Train loss: 1.173 | Val loss: 1.301 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny----------------\n",
      "Epoch:  36 | Train loss: 1.129 | Val loss: 1.281 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-EOSEOSEOSEOS---EOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyny-y--------------\n",
      "Epoch:  37 | Train loss: 1.104 | Val loss: 1.276 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-EOSEOSEOSEOS----EOSEOSEOSEOSEOS---- eyny-y--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  38 | Train loss: 1.088 | Val loss: 1.240 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOS-------------- eyni-i--------------\n",
      "Epoch:  39 | Train loss: 1.054 | Val loss: 1.219 | Gen: iyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyEOS-EOSEOSEOS------------- eyni----------------\n",
      "Epoch:  40 | Train loss: 1.030 | Val loss: 1.220 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSEOSEOSyEOSyEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSoEOSoEOSo eyEOSEOSEOSEOSEOS---ooo------- eyni----------------\n",
      "Epoch:  41 | Train loss: 1.011 | Val loss: 1.178 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSyEOSyEOSyEOSy oyEOSoEOSEOSEOSEOSEOSoooEOSoEOSoEOSoEOSo eyEOS-EOS-EOS------------- eyny----------------\n",
      "Epoch:  42 | Train loss: 0.996 | Val loss: 1.168 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSyEOSyEOSyEOSy oyEOSoEOSEOSEOSEOSEOSoooEOSoEOSoEOSoEOSo eyEOS-EOSEOSEOS------------- eygy----------------\n",
      "Epoch:  43 | Train loss: 0.971 | Val loss: 1.184 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSyEOSyEOSyEOSEOS oyEOSoEOSEOSEOSEOSEOSEOSEOSoEOSoEOSoEOSoEOSo eyEOS-EOSEOS-------------- eyg-----------------\n",
      "Epoch:  44 | Train loss: 0.947 | Val loss: 1.136 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSyyyiEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSoEOSoEOSoEOSoEOSEOSEOSEOS eyEOS-EOSEOSEOS------------- erg-----------------\n",
      "Epoch:  45 | Train loss: 0.943 | Val loss: 1.122 | Gen: iyEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOSEOSyiiEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSoooEOSoEOSEOSEOSEOSEOSEOS eyEOS-EOSEOSEOS------------- erg-----------------\n",
      "Epoch:  46 | Train loss: 0.909 | Val loss: 1.168 | Gen: iyEOSEOSEOSEOSEOSyiEOSEOSEOSyEOSyEOSyEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSoEOSoEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOS-EOSoooooooooooo erg---------EOS-EOS-EOS-EOS-\n",
      "Epoch:  47 | Train loss: 0.896 | Val loss: 1.109 | Gen: iyEOSEOSEOSEOSEOSyiEOSEOSEOSEOSEOSyEOSyEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSoooEOSoEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSEOSEOSEOSoooooooooo eig-----------------\n",
      "Epoch:  48 | Train loss: 0.892 | Val loss: 1.106 | Gen: iyEOSEOSEOSEOSEOSyiiEOSEOSEOSEOSyEOSyEOSEOSEOS oyEOSoEOSEOSEOSoEOSoooEOSEOSEOSEOSEOSEOSEOSo eyEOSEOSEOSEOSEOSEOSEOSEOSoooooooooo eig-----------------\n",
      "Epoch:  49 | Train loss: 0.855 | Val loss: 1.109 | Gen: iyEOSEOSEOSEOSEOSyiEOSEOSEOSyEOSyEOSyEOSEOSEOS oyEOSoEOSEOSEOSEOSEOSoooEOSEOSEOSEOSEOSooo eyEOSEOS-EOS-ooooooooooooo eig-------oiEOS-EOS-EOSoEOS-\n",
      "Epoch:  50 | Train loss: 0.861 | Val loss: 1.071 | Gen: iyEOSEOSEOSEOSEOSyiEOSEOSEOSyEOSyEOSyEOSEOSEOS oyEOSEOSEOSEOSEOSEOSEOSoEOSoooEOSEOSEOSEOSEOSo eyEOSEOS-EOS-ooooooooooooo eig-------ooEOS-EOSo-o-o\n",
      "Epoch:  51 | Train loss: 0.831 | Val loss: 1.077 | Gen: iyEOSEOSEOSEOSEOSyiiEOSEOSEOSEOSEOSEOSyiiEOS oyEOSoEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSo eyEOSEOSEOSEOSEOSEOSEOSooooooooooo eigg-------oii-o-ooo\n",
      "Epoch:  52 | Train loss: 0.815 | Val loss: 1.051 | Gen: iyEOSEOSEOSEOSEOSiiiEOSEOS-EOSEOSEOSoiyEOS oyEOSEOSEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSo eyEOSEOSEOSEOSEOSEOSEOSooooooooooo erg------ooooiii-ooo\n",
      "Epoch:  53 | Train loss: 0.805 | Val loss: 1.080 | Gen: iyEOSEOSEOSEOSEOSiiiEOSEOSEOSEOSEOSEOSoEOSyEOS oyEOSEOSEOSEOSEOSEOSEOSoooEOSoEOSEOSEOSEOSEOSo eyEOSEOSEOSEOSEOSEOSoooooooooooo ern------oooooiiEOSooo\n",
      "Epoch:  54 | Train loss: 0.787 | Val loss: 1.022 | Gen: iyEOSEOSEOSEOSEOSiiyEOSEOSEOSEOSEOSEOSEOSEOSyEOS oyEOSoEOSEOSEOSoooooEOSEOSEOSEOSoooo eyEOSEOSEOSEOSEOSooooooooooooo erg-----oooooooEOSoooo\n",
      "Epoch:  55 | Train loss: 0.771 | Val loss: 1.025 | Gen: iyEOSEOSEOSEOSEOSiiyEOSEOSEOSEOSEOSEOSEOSEOSyEOS oyEOSoEOSEOSEOSoooooEOSEOSEOSEOSoooo eyEOSEOSEOSEOSEOSEOSoooooooooooo erg------ooooii--ooo\n",
      "Epoch:  56 | Train loss: 0.751 | Val loss: 1.009 | Gen: iyEOSEOSEOSEOSEOSiwyEOSEOSEOSEOSEOSEOSEOSEOSyEOS oyEOSoEOSEOSEOSoooooEOSEOSEOSEOSEOSoEOSEOS eyEOSEOSEOSEOSEOSEOSooooooooooEOSEOS eig------oooiiiEOS-ooo\n",
      "Epoch:  57 | Train loss: 0.753 | Val loss: 1.029 | Gen: iyEOSEOSEOSEOSoiwyEOSEOSEOSEOSEOSEOSEOSEOSyEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOS eyEOSEOSEOSEOSEOSoooooooEOSEOSEOSEOSoo ein------oooiiEOSEOS-ooo\n",
      "Epoch:  58 | Train loss: 0.727 | Val loss: 0.997 | Gen: iyEOSEOSEOSEOSoiwyEOSEOSEOSEOSEOSEOSoiyEOS oyEOSoEOSEOSEOSoooooEOSEOSEOSoEOSooo eyEOSEOSEOSEOSEOSooooooooEOSEOSEOSEOSEOS ein------oooiiiEOS-ooo\n",
      "Epoch:  59 | Train loss: 0.708 | Val loss: 0.986 | Gen: iyEOSEOSEOSEOSoiiyEOSEOSEOSEOSoEOSiyEOSEOS oyEOSoEOSEOSEOSoooooEOSEOSEOSEOSoooo eyEOSEOS-ooooooooEOSEOSEOSEOSooo ein-----ooooiiEOS-oooo\n",
      "Epoch:  60 | Train loss: 0.703 | Val loss: 0.993 | Gen: iyEOSEOSEOSEOSoiiyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSoooo eEOSEOSEOS-ooooooooEOSEOSooooo ein-----ooooiiEOS-oooo\n",
      "Epoch:  61 | Train loss: 0.690 | Val loss: 0.959 | Gen: iyEOSEOSEOSEOSoiiyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSooooo eEOSEOSEOSEOSEOSooooooooEOSEOSEOSEOSoo ein-----ooooiiEOS-oooo\n",
      "Epoch:  62 | Train loss: 0.671 | Val loss: 0.965 | Gen: iyEOSEOSEOSEOSoiiyEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSooooo eEOSEOSEOSEOSEOSEOSEOSEOSooooooooooEOS ein-----ooooiiEOS-oooo\n",
      "Epoch:  63 | Train loss: 0.664 | Val loss: 0.949 | Gen: iyEOSEOSEOSEOSoiiyEOSEOSEOSEOSoiiEOSEOSEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSooooo eyEOSEOSEOSEOSEOSEOSEOSooooooooEOSEOSEOS ein-----ooooiiEOS-oooo\n",
      "Epoch:  64 | Train loss: 0.645 | Val loss: 0.947 | Gen: iyEOSEOSEOSEOSoiwyEOSEOSEOSEOSEOSEOSEOSiyEOS oyEOSEOSEOSEOSEOSoooooEOSEOSoooooo eyEOSEOSEOSEOSEOSEOSooooooooEOSEOSEOSEOS ei------oooiiEOSEOSooooo\n",
      "Epoch:  65 | Train loss: 0.636 | Val loss: 0.926 | Gen: iiEOSEOSEOSEOSoiwiEOSEOSEOSEOSEOSEOSEOSEOSEOSi oyEOSEOSEOSEOSEOSooooyEOSEOSooooiEOS eyEOSEOSEOSEOSEOSEOSoooooooEOSEOSEOSEOSEOS ein-----ooooiEOSEOSioooo\n",
      "Epoch:  66 | Train loss: 0.625 | Val loss: 0.909 | Gen: iiEOSEOSEOSEOSoiiiEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSooooyEOSEOSEOSooooEOS eEOSEOSEOSEOSooooooooEOSEOSEOSEOSEOSEOSo ein-----ooooiiiEOSiooo\n",
      "Epoch:  67 | Train loss: 0.607 | Val loss: 0.930 | Gen: iiEOSEOSEOSEOSoiiiEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSooooEOSEOSEOSEOSooooEOS eEOSEOSEOS-ooooEOSeyEOSEOSEOSEOSEOSEOSEOSEOS ein---EOSEOS-ooooiiEOSEOSooo\n",
      "Epoch:  68 | Train loss: 0.610 | Val loss: 0.916 | Gen: iiEOSEOSEOSEOSoiiiEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oylEOSEOSEOSEOSooooyEOSEOSEOSoooyEOS eEOSEOSEOS-oooooEOSyEOSEOSEOSEOSEOSEOSEOSEOS ein------ooooiiEOSEOSooo\n",
      "Epoch:  69 | Train loss: 0.583 | Val loss: 0.909 | Gen: iiEOSEOSEOSEOSoiiiyEOSEOSEOSEOSEOSEOSEOSiEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSoooo eEOSEOSEOSEOSEOSoooooooEOSEOSEOSEOSEOSEOSEOS ein------ooooiiEOSEOS--o\n",
      "Epoch:  70 | Train loss: 0.581 | Val loss: 0.908 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSEOSEOSEOSEOSooyEOS oyEOSEOSEOSEOSEOSEOSoooyEOSEOSEOSoooyEOS eEOSEOSEOSEOSEOSEOSooooooooEOSyEOSEOSEOS ei-------ooooiiEOSEOS--o\n",
      "Epoch:  71 | Train loss: 0.576 | Val loss: 0.919 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSoEOSEOSyyEOSEOSEOS oyEOSEOSEOSEOSEOSEOSoooyEOSEOSEOSoooyEOS eyEOSEOSEOSEOSEOSEOSEOSooooooeyEOSEOSEOS ey-------ooooiiiEOSEOSoo\n",
      "Epoch:  72 | Train loss: 0.555 | Val loss: 0.883 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oylEOSEOSEOSEOSEOSoooyEOSEOSooooyEOS eyEOSEOSEOSEOSEOSEOSooooogyEOSEOSEOSEOSEOS ey--------ooooiiEOSEOSoo\n",
      "Epoch:  73 | Train loss: 0.549 | Val loss: 0.886 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSEOSEOSEOSEOSEOSEOSoa oylEOSEOSEOSEOSooooyEOSEOSooooEOSEOS eyEOSEOSEOSEOSEOSoooooocEOSEOSEOSEOSEOSEOS ey--------ooooinEOSEOS-EOS\n",
      "Epoch:  74 | Train loss: 0.543 | Val loss: 0.880 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSEOSiEOSEOSEOSEOSoa oyEOSEOSEOSEOSEOSooooEOSEOSEOSoooooEOS eyEOSEOSEOSEOSEOSEOSooooogEOSEOSEOSEOSEOSEOS ey--------ooooiiEOSEOSoo\n",
      "Epoch:  75 | Train loss: 0.543 | Val loss: 0.879 | Gen: iiEOSEOSEOSEOSoiiEOSEOSEOSEOSoiEOSEOSEOSEOSEOS oyEOSEOSEOSEOSEOSooooEOSEOSEOSEOSoooEOSEOS eEOSEOSEOSEOSEOSoooooEOSeyEOSEOSEOSEOSEOSEOS eyn-----EOS-ooooiiEOSEOSoo\n",
      "Epoch:  76 | Train loss: 0.521 | Val loss: 0.870 | Gen: iiEOSEOSEOSEOSoiEOSEOSEOSoEOSoiEOSEOSEOSEOSEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSoooEOS eEOSEOSEOSEOSEOSEOSooooogEOSEOSEOSEOSEOSEOSEOS eyn-----EOSEOSoooooinEOSEOSEOS\n",
      "Epoch:  77 | Train loss: 0.525 | Val loss: 0.873 | Gen: iiEOSEOSEOSEOSoiEOSEOSEOSooaiEOSEOSEOSEOSEOS oylEOSEOSEOSEOSEOSooooyEOSEOSEOSoooEOS eEOSEOSEOSEOSEOSEOSooooogyyEOSEOSEOSEOSEOS eynn----EOSEOSEOSoooooniEOSEOS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  78 | Train loss: 0.505 | Val loss: 0.869 | Gen: iiEOSEOSEOSEOSoiEOSEOSEOSooiiEOSEOSEOSEOSEOS oylEOSEOSEOSEOSEOSoooyEOSEOSEOSEOSEOSooo eEOSEOSEOSEOSEOSEOSooooogyyEOSEOSEOSEOSEOS eyn-----EOSEOSooooognEOSEOSEOS\n",
      "Epoch:  79 | Train loss: 0.509 | Val loss: 0.875 | Gen: iiEOSEOSEOSEOSoiEOSEOSEOSooiiEOSEOSEOSoEOS oylEOSEOSEOSEOSEOSoooyEOSEOSoooEOSEOSEOS eEOSEOSEOSEOSEOSEOSooooogyyEOSEOSEOSEOSEOS ey------EOSEOSooooonnEOSEOSEOS\n",
      "Epoch:  80 | Train loss: 0.490 | Val loss: 0.864 | Gen: iiEOSEOSEOSooiiEOSEOSooiiEOSEOSEOSoEOS oylEOSEOSEOSEOSEOSoooyEOSEOSEOSoooyEOS eEOSEOSEOSEOSEOSEOSooooocyEOSEOSEOSEOSEOSEOS ey-------EOSEOSoooooonEOSEOS\n",
      "Epoch:  81 | Train loss: 0.490 | Val loss: 0.851 | Gen: iiEOSEOSoooiiEOSEOSoooiyEOSEOSEOSEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSoooEOS eEOSEOSEOSEOSEOSEOSoooooEOSyEOSEOSEOSEOSEOSEOS eyg-----EOSEOSEOSoooooogiEOS\n",
      "Epoch:  82 | Train loss: 0.469 | Val loss: 0.867 | Gen: iiEOSEOSoooiiEOSEOSEOSoEOSoEOSEOSEOSEOSo oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSooyEOS eEOSEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOS eyn----EOSEOSEOSooooognnEOSEOS\n",
      "Epoch:  83 | Train loss: 0.483 | Val loss: 0.865 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooyEOSEOSEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSooe eEOSEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOS eyn----EOSEOSEOSooooonnEOSEOSEOS\n",
      "Epoch:  84 | Train loss: 0.459 | Val loss: 0.838 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooEOSEOSEOSEOS oylEOSEOSEOSEOSEOSEOSooooEOSEOSEOSEOSooo eEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOSEOS eyn-----EOSEOSEOSooooonnEOSEOS\n",
      "Epoch:  85 | Train loss: 0.458 | Val loss: 0.850 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooEOSEOSEOSEOS oylEOSEOSEOSEOSEOSEOSoooyEOSEOSEOSolEOSEOS eEOSEOSEOSEOSEOSooooocEOSEOSEOSEOSEOSEOSEOSEOS ein------EOSEOSooooonnnEOS\n",
      "Epoch:  86 | Train loss: 0.444 | Val loss: 0.861 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooEOSEOSEOSo oylEOSEOSEOSEOSEOSoooyEOSEOSooEOSEOSEOSEOS eEOSEOSEOSEOSEOSooooocyEOSEOSEOSEOSEOSEOSEOS eyn-----EOSEOSooooooniEOSEOS\n",
      "Epoch:  87 | Train loss: 0.452 | Val loss: 0.873 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooEOSEOSEOSEOS oylEOSEOSEOSEOSooooEOSEOSEOSEOSEOSooyEOS eEOSEOSEOSEOSEOSooooEOScEOSEOSEOSEOSEOSEOSEOSEOS ey------EOSEOSoooooniiEOSEOS\n",
      "Epoch:  88 | Train loss: 0.431 | Val loss: 0.846 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSooEOSEOSEOSEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSolEOS eEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyg------EOSEOSooooonnnEOS\n",
      "Epoch:  89 | Train loss: 0.434 | Val loss: 0.846 | Gen: iiEOSEOSooowiEOSEOSEOSEOSEOSoEOSEOSEOSoEOS oylEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSyEOSEOS eEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyl------EOSEOSooooonnnEOS\n",
      "Epoch:  90 | Train loss: 0.422 | Val loss: 0.834 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSEOSyEOSEOSEOSEOSEOS oylEOSEOSEOSEOSoooyEOSEOSEOSEOSooooy eEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey------EOSEOSo-ooooniig\n",
      "Epoch:  91 | Train loss: 0.423 | Val loss: 0.873 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSEOSiyEOSEOSEOSEOS oyEOSEOSEOSEOSooooyEOSEOSEOSEOSEOSEOSEOSoo eEOSEOSEOSEOSEOSoooEOSEOSEOSEOSoooEOSEOSEOSEOS ey-----EOSEOSEOSooooiniEOSEOSEOS\n",
      "Epoch:  92 | Train loss: 0.419 | Val loss: 0.834 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSoiEOSEOSEOSEOSEOS oyEOSEOSEOSEOSooooyEOSEOSEOSEOSlEOSEOSEOSEOS eEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS ey------EOSEOSEOS-oooooonEOS\n",
      "Epoch:  93 | Train loss: 0.408 | Val loss: 0.850 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSooyEOSEOSEOSEOS oylEOSEOSEOSooooyEOSEOSEOSEOSoooEOSEOS eEOSEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyl------EOSEOSo-oooonii\n",
      "Epoch:  94 | Train loss: 0.411 | Val loss: 0.835 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSoooyEOSEOSEOS oylEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOSEOS eEOSEOSEOSEOSEOSoooEOSEOSEOSoEOSEOSEOSEOSEOSoEOS eyl------EOSEOSooooonniEOS\n",
      "Epoch:  95 | Train loss: 0.392 | Val loss: 0.845 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSEOSoooEOSEOSEOS oylEOSEOSEOSooooooEOSEOSEOSEOSEOSEOSEOSEOS eEOSEOSEOSEOSEOSEOSooEOSEOSEOSEOSEOSEOSEOSoEOSEOSEOS ey------EOSEOSooooinniEOSEOS\n",
      "Epoch:  96 | Train loss: 0.400 | Val loss: 0.844 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSEOSoooEOSEOSEOS oylEOSEOSEOSooooooEOSEOSEOSEOSEOSooo eEOSEOSEOSEOSEOSEOSooEOSEOSEOSEOSEOSiEOSEOSEOSEOSo eyn-----EOSEOSooooioniEOSEOS\n",
      "Epoch:  97 | Train loss: 0.382 | Val loss: 0.826 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSEOSEOSooyEOSEOS oylEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSoo eEOSEOSEOSEOSEOSoooEOSEOSEOSoEOSEOSEOSEOSEOSEOSEOS eyn-----EOSEOSEOSoooooniEOSEOS\n",
      "Epoch:  98 | Train loss: 0.385 | Val loss: 0.826 | Gen: iiEOSEOSooowEOSEOSEOSEOSEOSoEOSoyoEOSEOS oyEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOSEOS eEOSEOSEOSEOSEOSoooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyn------EOSEOSEOSooooiiii\n",
      "Epoch:  99 | Train loss: 0.376 | Val loss: 0.815 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSoEOSoyEOSEOSEOS oyEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSoooEOS eEOSEOSEOSEOSEOSoooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eyn-------EOSEOSo-ioooii\n",
      "Epoch: 100 | Train loss: 0.373 | Val loss: 0.843 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSoEOSEOSEOSEOSEOSEOS oyEOSEOSEOSEOSooooEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS eEOSEOSEOSEOSEOSooocyEOSEOSEOSEOSEOSEOSEOSEOSEOS ey--------EOSEOSo-EOSoooii\n",
      "Epoch: 101 | Train loss: 0.371 | Val loss: 0.819 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSoEOSoEOSoEOSEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOS eEOSEOSEOSEOSEOSooocyEOSEOSEOSEOSEOSEOSEOSEOSEOS ey-------EOSEOS-ooiiiiii\n",
      "Epoch: 102 | Train loss: 0.359 | Val loss: 0.823 | Gen: iiEOSEOSoooiEOSEOSEOSEOSEOSEOSEOSEOSEOSoEOSEOS oylEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSooo eEOSEOSEOSEOSEOSoooEOSEOSEOSEOSEOSEOSEOSoEOSEOSEOS eyn------EOSEOSEOSoooonEOSEOSEOS\n",
      "Epoch: 103 | Train loss: 0.366 | Val loss: 0.818 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSEOSEOSoEOSEOS oylEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSool eEOSEOSEOSEOSEOSoooEOSEOSEOSEOSooEOSEOSEOSoEOS ei-------EOS--o-oon-iEOS\n",
      "Epoch: 104 | Train loss: 0.346 | Val loss: 0.824 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSEOSEOSoEOSEOS oyEOSEOSEOSEOSEOSooooooEOSEOSEOSEOSEOSyEOS eEOSEOSEOSEOSEOSoooEOSEOSEOSooEOSEOSEOSEOSEOSEOS ey-------EOSEOS-iiioiiii\n",
      "Epoch: 105 | Train loss: 0.354 | Val loss: 0.843 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSoooyEOS oyEOSEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSo eEOSEOSEOSEOSEOSoooEOSEOSEOSoooEOSEOSEOSEOSEOS ey--i-----EOSEOS--EOSEOSooii\n",
      "Epoch: 106 | Train loss: 0.344 | Val loss: 0.813 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSoobEOSEOS oyEOSEOSEOSEOSoooooEOSEOSEOSEOSoEOSooo eEOSEOSEOSEOSEOSoooEOSiEOSEOSeEOSEOSEOSEOSEOSEOS ey--------EOS---EOSiiiii\n",
      "Epoch: 107 | Train loss: 0.338 | Val loss: 0.814 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSEOSobEOSEOS oyEOSEOSEOSEOSoooooEOSEOSEOSEOSEOSoooo eeEOSEOSEOSEOSoooEOSeEOSEOSlEOSEOSEOSEOSEOSEOS eyn-------EOS---iiiiii\n",
      "Epoch: 108 | Train loss: 0.341 | Val loss: 0.799 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSooooEOSEOS oyEOSEOSEOSoooooEOSEOSEOSEOSEOSEOSEOSEOSEOSo eeEOSEOSEOSEOSoooEOSEOSEOSEOSEOSyEOSdEOSEOSEOS eyn------EOSEOS-iiiiiiii\n",
      "Epoch: 109 | Train loss: 0.324 | Val loss: 0.806 | Gen: iiEOSooooiEOSEOSiEOSEOSEOSEOSoooyEOS oyEOSEOSEOSEOSooooEOSEOSEOSEOSooEOSosEOS eEOSEOSEOSEOSEOSoooEOSiEOSEOSeEOSEOSEOSEOSEOSEOS ey-------EOSEOSEOSEOSiiiiiii\n",
      "Epoch: 110 | Train loss: 0.330 | Val loss: 0.825 | Gen: iiEOSooooiEOSEOSiEOSEOSEOSEOSoooyEOS oyEOSEOSEOSEOSooooEOSEOSEOSEOSoolysEOS eEOSEOSEOSEOSEOSoooEOSiEOSlEOSEOSEOSEOSEOSEOSEOS ey--i----EOSEOSEOSiEOSEOSiiiii\n",
      "Epoch: 111 | Train loss: 0.321 | Val loss: 0.800 | Gen: iiEOSooooiEOSEOSEOSEOSEOSEOSEOSoEOSEOSEOSEOS oyEOSEOSEOSooooolEOSEOSEOSEOSEOSEOSoEOSo eEOSEOSEOSEOSEOSoooEOSEOSooEOSeEOSEOSEOSEOSEOS ey-------EOSEOSEOSiiiiiiii\n",
      "Epoch: 112 | Train loss: 0.313 | Val loss: 0.804 | Gen: iiEOSoooowEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooolEOSEOSEOSoEOSEOSoEOSEOS eeEOSEOSEOSEOSoooEOSEOSEOSEOSoEOSEOSEOSEOSEOSEOS ey-------EOSEOS--iiiiiii\n",
      "Epoch: 113 | Train loss: 0.318 | Val loss: 0.804 | Gen: iiEOSoooowEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooolEOSEOSEOSEOSoooob eeEOSEOSEOSEOSoooEOSEOSooEOSiEOSEOSEOSEOSEOS ey--------EOS---iiiiii\n",
      "Epoch: 114 | Train loss: 0.304 | Val loss: 0.803 | Gen: iiEOSoooifEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooloEOSEOSEOSEOSEOSEOSEOSoo eeEOSEOSEOSEOSoooEOSEOSoEOSEOSEOSEOSoEOSEOSEOS ey--i-----EOS-EOS-iiiiii\n",
      "Epoch: 115 | Train loss: 0.306 | Val loss: 0.805 | Gen: iiEOSoooiwEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooloEOSEOSEOSEOSEOSEOSEOSEOSo eEOSEOSEOSEOSEOSoEOSoEOSEOSoEOSEOSeEOSEOSEOSEOSEOS ey--i-----EOSEOSEOS-iiiiii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 | Train loss: 0.302 | Val loss: 0.766 | Gen: iiEOSoooiwEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooloEOSEOSEOSEOSEOSEOSEOSEOSEOS eeEOSEOSEOSEOSEOSEOSoEOSEOSoEOSEOSiEOSEOSEOSEOSEOS ey--i-----EOSEOSEOSiiiiiii\n",
      "Epoch: 117 | Train loss: 0.294 | Val loss: 0.760 | Gen: iiEOSoooiwEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooooEOSEOSEOSEOSoEOSEOSEOSo eeEOSEOSEOSEOSEOSEOSoodEOSEOSoEOSEOSEOSEOSoEOS ey-------EOS-iiiiiiiii\n",
      "Epoch: 118 | Train loss: 0.298 | Val loss: 0.775 | Gen: iiEOSoooiwEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooolEOSEOSEOSEOSEOSEOSEOSEOSo eeEOSEOSEOSEOSoooodEOSEOSoEOSEOSEOSEOSoEOS ei-------EOSEOSiiiiiiiii\n",
      "Epoch: 119 | Train loss: 0.287 | Val loss: 0.793 | Gen: iiEOSoooiwEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOSEOS oyEOSEOSEOSooooolEOSEOSEOSEOSEOSEOSEOSoEOS eeEOSEOSEOSEOSoooEOSEOSoEOSEOSeEOSEOSEOSEOSEOS ei--i------EOSEOSEOSiiiiii\n",
      "Exiting early from training.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_model(train_dict, val_dict, idx_dict, encoder, decoder, opts)\n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting early from training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
